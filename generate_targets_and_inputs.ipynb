{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generate_targets_and_inputs.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/freddejn/summarization-transformer-cnn-dailymail/blob/master/generate_targets_and_inputs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW_8TFgoACZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "!pip install -q -U tensor2tensor\n",
        "import tensorflow as tf\n",
        "from tensor2tensor.utils import trainer_lib, registry\n",
        "from tensor2tensor import problems\n",
        "from tensor2tensor.data_generators import problem\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from google.colab import auth\n",
        "import os\n",
        "tfe = tf.contrib.eager\n",
        "tfe.enable_eager_execution()\n",
        "\n",
        "auth.authenticate_user()\n",
        "Modes = tf.estimator.ModeKeys\n",
        "\n",
        "PROJECT_ID = 'transformer-233711'\n",
        "!gcloud config set project {PROJECT_ID}\n",
        "BUCKET = 'tensor2tensor-test-bucket'\n",
        "DATA_DIR = f'gs://{BUCKET}/data'\n",
        "PROBLEM_NAME = 'summarize_cnn_dailymail32k'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnZj_0p2AN3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Generates input and targets for later evaluation\n",
        "import re\n",
        "DATASET = 'eval'        # Select to generate from test or eval set.\n",
        "MAX_LENGTH = 512        # Input will be truncated to this length.\n",
        "MAX_TARGET_LENGTH = 128 # Targets longer than this in sub-words will be dropped.\n",
        "MAX_TESTS = 10          # How many samples to generate.\n",
        "trunk = True            # Select if to truncate or drop.\n",
        "all_inputs = []\n",
        "all_targets = []\n",
        "\n",
        "# Picks random samples from array, uses num as seed\n",
        "def get_random_samples(arr, num):\n",
        "    np.random.seed(num)\n",
        "    return np.random.choice(arr, num, replace=False)\n",
        "\n",
        "# Decoding input of integers to text for the t2t-decoder\n",
        "def decode(integers, encoders):\n",
        "    integers = list(np.squeeze(integers))\n",
        "    if 1 in integers:\n",
        "        integers = integers[:integers.index(1)]\n",
        "    return encoders[\"inputs\"].decode(np.squeeze(integers))\n",
        "\n",
        "# Save array to file, line by line\n",
        "def save_arr(arr, filename):\n",
        "    with open(filename, 'w') as file:\n",
        "        for txt in arr:\n",
        "            file.write(f'{txt}\\n')\n",
        "            \n",
        "# Will generate examples one by one\n",
        "summarize_problem = problems.problem(PROBLEM_NAME)\n",
        "encoders = summarize_problem.feature_encoders(DATA_DIR)\n",
        "if DATASET == 'test':\n",
        "    eval_data_dir = f'gs://{BUCKET}/data_for_test'\n",
        "    predict_data_examples = tfe.Iterator(summarize_problem.dataset(problem.DatasetSplit.TEST, DATA_DIR))\n",
        "if DATASET == 'eval':\n",
        "    eval_data_dir = f'gs://{BUCKET}/data_for_evaluation'\n",
        "    predict_data_examples = tfe.Iterator(summarize_problem.dataset(problem.DatasetSplit.EVAL, DATA_DIR))\n",
        "\n",
        "count = 0\n",
        "if trunk:\n",
        "    for _, data in enumerate(predict_data_examples):\n",
        "        if  data['targets'].shape[0] <= MAX_TARGET_LENGTH:\n",
        "            len_inputs = data['inputs'].shape[0]\n",
        "            data['inputs'] = data['inputs'][:MAX_LENGTH]\n",
        "            count+=1\n",
        "            all_inputs.append(decode(data['inputs'], encoders))\n",
        "            all_targets.append(decode(data['targets'], encoders))\n",
        "            print(f'{count}.({all_inputs[count-1][0:20]}... ,{all_targets[count-1][0:10]}...)', end=\"\\t\")\n",
        "else:\n",
        "    for _, data in enumerate(predict_data_examples):\n",
        "    # Here inputs could be manually filtered on max length\n",
        "        if data['inputs'].shape[0] <= MAX_LENGTH and data['targets'].shape[0] <= MAX_TARGET_LENGTH :\n",
        "            count+=1\n",
        "            all_inputs.append(decode(data['inputs'], encoders))\n",
        "            all_targets.append(decode(data['targets'], encoders))\n",
        "            print(f'{count}.({all_inputs[count-1][0:20]}... ,{all_targets[count-1][0:10]}...)', end=\"\\t\")\n",
        "\n",
        "        \n",
        "# Randomize samples\n",
        "all_inputs = get_random_samples(all_inputs, MAX_TESTS)\n",
        "all_targets = get_random_samples(all_targets, MAX_TESTS)\n",
        "\n",
        "# Print out how many samples generated\n",
        "print(f'\\nNum Inputs: {len(all_inputs)}\\nNum Targets: {len(all_targets)}')\n",
        "\n",
        "# Store locally before copying to gcs bucket\n",
        "save_arr(all_inputs, 'all_inputs.txt')\n",
        "save_arr(all_targets, 'all_targets.txt')\n",
        "\n",
        "# Copy targets and inputs to gs bucket, label by max_length and max_test, one example per line\n",
        "if trunk:\n",
        "    !gsutil cp 'all_inputs.txt' '{eval_data_dir}/trunk_{MAX_LENGTH}_num_{MAX_TESTS}_inputs.txt'\n",
        "    !gsutil cp 'all_targets.txt' '{eval_data_dir}/trunk_{MAX_LENGTH}_num_{MAX_TESTS}_targets.txt'\n",
        "    \n",
        "else:\n",
        "    !gsutil cp 'all_inputs.txt' '{eval_data_dir}/len_{MAX_LENGTH}_num_{MAX_TESTS}_inputs.txt'\n",
        "    !gsutil cp 'all_targets.txt' '{eval_data_dir}/len_{MAX_LENGTH}_num_{MAX_TESTS}_targets.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}