{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataset-metadata.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/freddejn/summarization-transformer-cnn-dailymail/blob/master/dataset_metadata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqHToxX46ois",
        "colab_type": "text"
      },
      "source": [
        "# Display information about dataset.\n",
        "\n",
        "* Runs through entire dataset and extracts input and output lenghts\n",
        "* Stores data in dataframe on GCS bucket `DATASET_METADATA_PATH/type-metadata.csv`, where type is of train test or eval.\n",
        "* Path to training data is specifyed by `DATA_DIR`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itPl23eoQYE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "import tensorflow as tf \n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "!pip install -q -U tensor2tensor\n",
        "from tensor2tensor import problems \n",
        "from tensor2tensor.data_generators import problem\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "PROJECT_ID = 'transformer-233711'\n",
        "!gcloud config set project {PROJECT_ID}\n",
        "BUCKET = 'tensor2tensor-test-bucket'\n",
        "\n",
        "PROBLEM_NAME = 'summarize_cnn_dailymail32k'\n",
        "MODES = tf.estimator.ModeKeys\n",
        "DATA_DIR = f'gs://{BUCKET}/data'\n",
        "DATASET_METADATA_PATH = f'gs://{BUCKET}/metadata/'\n",
        "\n",
        "tfe = tf.contrib.eager \n",
        "tfe.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwcjQ65fRBhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Runs through dataset and stores in GCS, checks if already exists on bucket\n",
        "# before running.\n",
        "def data_exists(file, path):\n",
        "    files = !gsutil ls $path\n",
        "    return (path + file) in files\n",
        "\n",
        "summary_problem = problems.problem(PROBLEM_NAME) \n",
        "encoders = summary_problem.feature_encoders(DATA_DIR)\n",
        "\n",
        "example_iterators = { 'eval': tfe.Iterator(summary_problem.dataset(problem.DatasetSplit.EVAL, DATA_DIR)),\n",
        "                      'test': tfe.Iterator(summary_problem.dataset(problem.DatasetSplit.TEST, DATA_DIR)),\n",
        "                      'train':tfe.Iterator(summary_problem.dataset(problem.DatasetSplit.TRAIN, DATA_DIR))}\n",
        "\n",
        "for key, example_iterator in example_iterators.items():\n",
        "    print(f'Extracting metadatat {key}')\n",
        "    tmp_file = f'{key}-metadata.csv'\n",
        "    input_shapes = []\n",
        "    output_shapes = []\n",
        "    if data_exists(tmp_file, DATASET_METADATA_PATH):\n",
        "        print(f'{key} data already exists')\n",
        "        continue\n",
        "    for example in example_iterator:\n",
        "        input_shapes.append(example['inputs'].shape[0])\n",
        "        output_shapes.append(example['targets'].shape[0])\n",
        "    data = pd.DataFrame({'input_shape':input_shapes, 'output_shape':output_shapes})\n",
        "    data.to_csv(tmp_file)\n",
        "    !gsutil cp $tmp_file $DATASET_METADATA_PATH\n",
        "    display(data.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNtn6xM19J27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy files locally if not already done\n",
        "!gsutil cp $DATASET_METADATA_PATH'eval-metadata.csv' 'eval-metadata.csv'\n",
        "!gsutil cp $DATASET_METADATA_PATH'test-metadata.csv' 'test-metadata.csv'\n",
        "!gsutil cp $DATASET_METADATA_PATH'train-metadata.csv' 'train-metadata.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OonF0bdVRJPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display length of each dataset.\n",
        "eval_data = pd.read_csv('eval-metadata.csv')\n",
        "test_data = pd.read_csv('test-metadata.csv')\n",
        "train_data = pd.read_csv('train-metadata.csv')\n",
        "eval_articles = eval_data.shape[0]\n",
        "test_articles = test_data.shape[0]\n",
        "train_articles = train_data.shape[0]\n",
        "total_articles = eval_articles + train_articles + test_articles\n",
        "print(100*'-' + '\\n')\n",
        "print(f'Number of train samples: {train_articles:,} ({train_articles/total_articles:.1%}) \\n' \n",
        "        f'Number of evaluation samples {eval_articles:,}({eval_articles/total_articles:.1%})\\n'\n",
        "        f'Number of test samples: {test_articles:,} ({test_articles/total_articles:.1%})\\n'\n",
        "        f'Total samples {total_articles:,} ({total_articles/total_articles:0.0%})\\n') \n",
        "\n",
        "display(f'Eval: {eval_data.shape}',eval_data.head(5))\n",
        "display(f'Test: {test_data.shape}', test_data.head(5))\n",
        "display(f'Train: {train_data.shape}', train_data.head(5))\n",
        "display(train_data.mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSskG2kXRM7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print bin sizes and display min and max lenghts of data.\n",
        "import numpy as np\n",
        "\n",
        "bins = pd.DataFrame()\n",
        "bin_split = [0,128,256,512,1024,2048,4096,9999]\n",
        "all_data = pd.concat([eval_data, test_data, train_data], axis=0)\n",
        "input_binned = all_data.groupby(pd.cut(all_data['input_shape'], bins=bin_split)).size()\n",
        "output_binned = all_data.groupby(pd.cut(all_data['output_shape'], bins=bin_split)).size()\n",
        "bins['num_input'] = input_binned\n",
        "bins['num_output'] = output_binned\n",
        "bins.index.name = 'Bins'\n",
        "\n",
        "avg_in, avg_out = all_data.input_shape.mean(), all_data.output_shape.mean()\n",
        "min_in, min_out = all_data.input_shape.min(), all_data.output_shape.min()\n",
        "max_in, max_out = all_data.input_shape.max(), all_data.output_shape.max()\n",
        "bins['fraction_input'] = bins.num_input/bins.num_input.sum()\n",
        "bins['fraction_output'] = bins.num_output/bins.num_output.sum()\n",
        "bins['cum_fraction_input'] = bins.fraction_input.cumsum()\n",
        "bins['cum_fraction_output'] = bins.fraction_output.cumsum()\n",
        "display(bins)\n",
        "print(f'Average len input: {avg_in:.2f}\\n' \\\n",
        "      f'Average len output: {avg_out:.2f}\\n' \\\n",
        "      f'Max len input: {max_in}\\n' \\\n",
        "      f'Max len output: {max_out}\\n' \\\n",
        "      f'Min len input: {min_in}\\n' \\\n",
        "      f'Min len output: {min_out}\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVDzVKt_RPkl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "from matplotlib.ticker import PercentFormatter\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mlt\n",
        "colors = ['#3498db', '#95a5a6']\n",
        "\n",
        "fig, ax = plt.subplots(2,1, figsize=(13,8))\n",
        "for i, a in enumerate(ax):\n",
        "    a.spines['top'].set_visible(False)\n",
        "    a.spines['right'].set_visible(False)\n",
        "    a.set_ylabel('Fraction (â€°)', fontsize=18)\n",
        "    a.tick_params(labelsize=18)\n",
        "    a.yaxis.set_major_formatter(FuncFormatter(lambda x, pos: \"%.1f\" % (x*1000)))\n",
        "\n",
        "ax[0].hist(all_data.input_shape, bins=range(0,3072,32), density=True, \\\n",
        "           label='input', alpha=0.6, color=colors[0], edgecolor='black')\n",
        "ax[0].set_xticks(range(0,2560,512))\n",
        "ax[0].set_xlim(right=2500, left=0)\n",
        "ax[0].set_xlabel('Input Length', fontsize=18)\n",
        "\n",
        "ax[1].hist(all_data.output_shape, bins=range(0,256,2), density=True, \\\n",
        "           label='targets', alpha=0.6, color=colors[1], edgecolor='black')\n",
        "ax[1].set_xlim(right=170, left=0)\n",
        "ax[1].set_xticks(range(0,192,32))\n",
        "ax[1].set_xlabel('Target Length', fontsize=18)\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.savefig('data_histogram.pdf')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}