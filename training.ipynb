{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/freddejn/summarization-transformer-cnn-dailymail/blob/master/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fQuTUXiKgsoE",
        "colab": {}
      },
      "source": [
        "import os\n",
        "!pip install -q -U tensor2tensor\n",
        "!pip install -q -U tensorflow\n",
        "#!pip install -q tensor2tensor==1.13.2 --force-reinstall\n",
        "\n",
        "from google.colab import auth\n",
        "import tensorflow as tf\n",
        "from tensor2tensor.utils import registry\n",
        "\n",
        "PROJECT_ID = 'transformer-233711'\n",
        "!gcloud config set project {PROJECT_ID}\n",
        "auth.authenticate_user()\n",
        "!mkdir user_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyaWsJkLgf-m",
        "colab_type": "text"
      },
      "source": [
        "# Main program"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zgqRTLjPkMeq",
        "colab": {}
      },
      "source": [
        "%%writefile user_dir/registrations.py\n",
        "from tensor2tensor.utils import registry\n",
        "from tensor2tensor.models import transformer\n",
        "from tensor2tensor.models import lstm\n",
        "from tensor2tensor.data_generators import problem\n",
        "\n",
        "# transformer_base_v1\n",
        "# Used for machine translation in original paper.\n",
        "# Does not learn for summarization task due to layer_postprocess_sequence\n",
        "# layer_preprocess_sequence set to dan and none respectively.\n",
        "@registry.register_hparams\n",
        "def transformer_base_v1_extra():\n",
        "    hparams = transformer.transformer_base_v1()\n",
        "    hparams.batch_size = 4096\n",
        "    hparams.max_length = 2048\n",
        "    hparams.max_input_seq_length = 512\n",
        "    hparams.max_target_seq_length = 128\n",
        "    return hparams\n",
        "\n",
        "# transformer_base_v3\n",
        "# Default for the transformer base model.\n",
        "# Learns on summarization task but not as fast as transformer_tpu.\n",
        "@registry.register_hparams\n",
        "def transformer_base_v3_extra():\n",
        "    hparams = transformer.transformer_base_v3()\n",
        "    hparams.batch_size = 4096\n",
        "    hparams.max_length = 2048      # Use 512 for hparam-testing\n",
        "    hparams.max_input_seq_length = 512\n",
        "    hparams.max_target_seq_length = 128\n",
        "    return hparams\n",
        "\n",
        "\n",
        "# transformer_tpu\n",
        "# Optimized for running on tpu.\n",
        "# Trains fastest of the hyperparameters tested.\n",
        "@registry.register_hparams\n",
        "def transformer_tpu_extra():\n",
        "    hparams = transformer.transformer_tpu()\n",
        "    hparams.batch_size = 4096\n",
        "    hparams.max_length = 2048\n",
        "    hparams.max_input_seq_length = 512\n",
        "    hparams.max_target_seq_length = 128\n",
        "    return hparams\n",
        "\n",
        "\n",
        "# transformer_prepend\n",
        "# Results in good ROUGE-scores but copies input and uses it as output.\n",
        "@registry.register_hparams\n",
        "def transformer_prepend_extra():\n",
        "    hparams = transformer.transformer_prepend()\n",
        "    hparams_batch_size=4096\n",
        "    hparams.max_length = 2048\n",
        "    return hparams\n",
        "\n",
        "# lstm_bahdanau_attention\n",
        "# Uses more memory than luong_attention\n",
        "# 9GB with batch size 4096 and max_length 1024\n",
        "# 8+ GB with batch size 1024 and max_length 1024\n",
        "@registry.register_hparams\n",
        "def lstm_bahdanau_extra():\n",
        "    hparams = lstm.lstm_bahdanau_attention() # Uses 9GB memory for (4096, 1024)\n",
        "    hparams.batch_size = 4096                # Uses 8+ memory for (1024, 1024)\n",
        "    hparams.max_length = 2048\n",
        "    hparams.hidden_size = 128\n",
        "    hparams.max_input_seq_length = 512\n",
        "    hparams.max_target_seq_length = 128\n",
        "    return hparams\n",
        "\n",
        "# lstm_luong_attention\n",
        "# More memory efficient than bahdanau_attention with no apparent difference in \n",
        "# ROUGE-score.\n",
        "# Works best with bidirectional encoder and hiddn size 256\n",
        "@registry.register_hparams\n",
        "def lstm_luong_extra():\n",
        "    hparams = lstm.lstm_luong_attention()\n",
        "    hparams.batch_size = 4096\n",
        "    hparams.max_length = 2048\n",
        "    hparams.hidden_size=256\n",
        "    hparams.max_input_seq_length = 512\n",
        "    hparams.max_target_seq_length = 128\n",
        "    return hparams\n",
        "\n",
        "@registry.register_ranged_hparams\n",
        "def ranged_lstm(hparams):\n",
        "    hparams.set_float(\"dropout\", 0.2, 0.4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_z7AWRIZG7-w",
        "colab": {}
      },
      "source": [
        "%%writefile user_dir/__init__.py\n",
        "from . import registrations as reg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T3U0NTn1hLvn",
        "colab": {}
      },
      "source": [
        "from user_dir import registrations as reg\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "# Change MODEL and OUTPUT_DIR\n",
        "run = reg.transformer_tpu_extra\n",
        "HPARAMS_SET = run.__name__\n",
        "BATCH_SIZE = run().batch_size\n",
        "MAX_LENGTH = run().batch_size\n",
        "MAX_INPUT_SEQ_LENGTH = run().max_input_seq_length\n",
        "MAX_TARGET_SEQ_LENGTH = run().max_target_seq_length\n",
        "OUTPUT_DIR = f'gs://tensor2tensor-test-bucket/{HPARAMS_SET}-b{BATCH_SIZE}-ml{MAX_LENGTH}-mi{MAX_INPUT_SEQ_LENGTH}-mt{MAX_TARGET_SEQ_LENGTH}'\n",
        "MODEL = 'transformer'\n",
        "\n",
        "\n",
        "!t2t-trainer \\\n",
        "  --data_dir='gs://tensor2tensor-test-bucket/data'\\\n",
        "  --output_dir=$OUTPUT_DIR \\\n",
        "  --t2t_usr_dir='/content/user_dir' \\\n",
        "  --problem='summarize_cnn_dailymail32k' \\\n",
        "  --model=$MODEL \\\n",
        "  --hparams_set=$HPARAMS_SET \\\n",
        "  --train_steps=1000000 \\\n",
        "  --eval_steps=10 \\\n",
        "  --local_eval_frequency=10000 \\\n",
        "  --use_tpu \\\n",
        "  --keep_checkpoint_max=300 \\\n",
        "  --cloud_tpu_name=$TPU_WORKER \\"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}