{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "decode-and-evaluate.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/freddejn/summarization-transformer-cnn-dailymail/blob/master/decode_and_evaluate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUCdf3YFUFke",
        "colab_type": "text"
      },
      "source": [
        "# Runs decoding from file and evaluates using ROUGE-155\n",
        "\n",
        "* Input should be files containing data one article per line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVroaFrueOQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q -U tensor2tensor\n",
        "!pip install -q -U tensorflow\n",
        "!pip install pyrouge\n",
        "!git clone https://github.com/andersjo/pyrouge.git\n",
        "!!pyrouge_set_rouge_path '/content/pyrouge/tools/ROUGE-1.5.5/'\n",
        "!sudo apt-get install libxml-parser-perl\n",
        "!rm '/content/pyrouge/tools/ROUGE-1.5.5/data/WordNet-2.0-Exceptions/WordNet-2.0.exc.db'\n",
        "!/content/pyrouge/tools/ROUGE-1.5.5/data/WordNet-2.0-Exceptions/buildExeptionDB.pl . exc /content/pyrouge/tools/ROUGE-1.5.5/data/WordNet-2.0-Exceptions/WordNet-2.0.exc.db\n",
        "!rm /content/pyrouge/tools/ROUGE-1.5.5/data/WordNet-2.0.exc.db\n",
        "!ln -s /content/pyrouge/tools/ROUGE-1.5.5/data/WordNet-2.0-Exceptions/WordNet-2.0.exc.db /content/pyrouge/tools/ROUGE-1.5.5/data/WordNet-2.0.exc.db\n",
        "\n",
        "from google.colab import auth\n",
        "from nltk import tokenize\n",
        "from pyrouge import Rouge155\n",
        "from spacy.lang.en import English\n",
        "from tensor2tensor import models\n",
        "from tensor2tensor import problems\n",
        "from tensor2tensor.data_generators import problem\n",
        "from tensor2tensor.utils import trainer_lib, registry\n",
        "from tensor2tensor.utils import registry\n",
        "from tensor2tensor.utils import decoding\n",
        "from tensor2tensor.utils import hparams_lib\n",
        "from tensor2tensor.utils import trainer_lib\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "\n",
        "import datetime as dt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import sys\n",
        "\n",
        "en = English()\n",
        "en.add_pipe(en.create_pipe('sentencizer'))\n",
        "auth.authenticate_user()\n",
        "Modes = tf.estimator.ModeKeys\n",
        "\n",
        "PROJECT_ID = 'transformer-233711'\n",
        "!gcloud config set project {PROJECT_ID}\n",
        "BUCKET = 'gs://tensor2tensor-test-bucket'\n",
        "DATA_DIR = f'{BUCKET}/data'\n",
        "PROBLEM_NAME = 'summarize_cnn_dailymail32k'\n",
        "OUTPUT_DIR = 'output_dir/'\n",
        "TARGET_DIR = 'target_dir/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7NMwHHMybmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "# Read json file and return dict\n",
        "def get_json(file):\n",
        "    with open(file) as json_file:\n",
        "        return json.load(json_file)\n",
        "    \n",
        "def save_to_separate(file, path, prefix, suffix='.txt', output_lim=0):\n",
        "    with open(file, 'r') as infile:\n",
        "        for i, line in enumerate(infile.readlines()):\n",
        "            if output_lim:\n",
        "                line = ' '.join(line.split()[:output_lim]) # If output limit\n",
        "                if line:\n",
        "                    if line[-1] != \".\":\n",
        "                        line = line + \"...\"\n",
        "            line = en(line)\n",
        "            sentences = [sent.string.strip() for sent in line.sents]\n",
        "            with open(path + f'{prefix}{i:03d}{suffix}','w') as outfile:\n",
        "                for s in sentences:   \n",
        "                    outfile.write(f'{s}\\n')\n",
        "\n",
        "# Model class storing model-run information\n",
        "class Model:\n",
        "    def __init__(self, model_name, hparams_set, model_dir):\n",
        "        self.model_name = model_name\n",
        "        self.hparams_set = hparams_set\n",
        "        self.model_dir = model_dir\n",
        "        print(self.model_dir)\n",
        "        \n",
        "    def __str__(self):\n",
        "        return self.model_dir\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return self.__str__()\n",
        "    \n",
        "    def get_label(self):\n",
        "        return os.path.basename(self.model_dir)\n",
        "    \n",
        "# Deletes a tf.flag if set\n",
        "def delete_flags_if_set(names):\n",
        "    for name in names:\n",
        "        if(name in tf.app.flags.FLAGS._flags()):\n",
        "            tf.app.flags.FLAGS.__delattr__(name)\n",
        "\n",
        "# Decodes from file\n",
        "def decoder(run):\n",
        "    tf.logging.set_verbosity(tf.logging.INFO)\n",
        "    hp = hparams_lib.create_hparams(\n",
        "        hparams_set=run.hparams_set, \n",
        "        hparams_overrides_str='',\n",
        "        data_dir=DATA_DIR,\n",
        "        problem_name=PROBLEM_NAME\n",
        "    ) \n",
        "    decode_hp = decoding.decode_hparams('beam_size=4,alpha=0.6,log_results=False')\n",
        "    run_config = trainer_lib.create_run_config(\n",
        "        model_name=run.model_name, \n",
        "        cloud_tpu_name=TPU_WORKER,\n",
        "        use_tpu=True,\n",
        "        model_dir=run.model_dir \n",
        "    ) \n",
        "\n",
        "    estimator = trainer_lib.create_estimator(\n",
        "        model_name=run.model_name,\n",
        "        hparams=hp,\n",
        "        run_config=run_config,\n",
        "        decode_hparams=decode_hp,\n",
        "        use_tpu=True,\n",
        "        use_tpu_estimator=True \n",
        "    )\n",
        "    decoding.decode_from_file(estimator, filename=decode_from_file, hparams=hp, decode_hp=decode_hp, decode_to_file=decode_to_file)\n",
        "    \n",
        "def run_rouge_155(run):\n",
        "    r = Rouge155()\n",
        "    r.system_dir = OUTPUT_DIR\n",
        "    r.model_dir = TARGET_DIR\n",
        "    r.system_filename_pattern = 'file.(\\d+).txt'\n",
        "    r.model_filename_pattern = 'file.[A-Z].#ID#.txt'\n",
        "    output = r.convert_and_evaluate()\n",
        "    return r.output_to_dict(output)\n",
        "\n",
        "delete_flags_if_set(['f', 'problem'])\n",
        "tf.app.flags.DEFINE_string('problem', PROBLEM_NAME, \"Problem name.\")\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYEsz-Tzyqt3",
        "colab_type": "text"
      },
      "source": [
        "# Run inference and score using ROUGE 1.5.5\n",
        "### Set up all files before running inference, only needs to run once before infering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5z3C10euyneh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Path to trained models\n",
        "DATASET = 'eval'\n",
        "MAX_TESTS = 10\n",
        "MODEL_MAX_LENGTH = 2048\n",
        "TEST_MAX_LENGTH = 512\n",
        "OUTPUT_LIM = 128\n",
        "trunk = True\n",
        "\n",
        "if DATASET == 'test':\n",
        "    eval_data_dir = f'{BUCKET}/data_for_test'\n",
        "if DATASET == 'eval':\n",
        "    eval_data_dir = f'{BUCKET}/data_for_evaluation'\n",
        "    \n",
        "if trunk:\n",
        "    results_file = f'rouge-155-data-trunk-{DATASET}-set.csv'\n",
        "    decode_from_file = f'{eval_data_dir}/trunk_{TEST_MAX_LENGTH}_num_{MAX_TESTS}_inputs.txt'\n",
        "    targets = f'{eval_data_dir}/trunk_{TEST_MAX_LENGTH}_num_{MAX_TESTS}_targets.txt'\n",
        "else:\n",
        "    results_file = f'rouge-155-data-{DATASET}-set.csv'\n",
        "    decode_from_file = f'{eval_data_dir}/len_{TEST_MAX_LENGTH}_num_{MAX_TESTS}_inputs.txt'\n",
        "    targets = f'{eval_data_dir}/len_{test_max_length}_num_{MAX_TESTS}_targets.txt'\n",
        "\n",
        "results_path = f'{eval_data_dir}/{results_file}'\n",
        "\n",
        "\n",
        "# Copy targets and inputs to gs bucket, label by max_length and max_test, one example per line\n",
        "!gsutil cp '{decode_from_file}' 'all_inputs.txt' \n",
        "!gsutil cp '{targets}' 'all_targets.txt' \n",
        "\n",
        "models_to_evaluate = [\n",
        "    Model(model_name='transformer', hparams_set='transformer_tpu',\n",
        "      model_dir=f'{BUCKET}/transformer_tpu_extra-b4096-ml4096-mi512-mt128')\n",
        "]\n",
        "\n",
        "# Copy csv-file and targets file locally\n",
        "!gsutil cp '{results_path}' '{results_file}'\n",
        "!rm -rf {TARGET_DIR} && mkdir {TARGET_DIR}\n",
        "save_to_separate(\"all_targets.txt\", TARGET_DIR, 'file.A.')\n",
        "print(models_to_evaluate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7toU5t6PHbVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Run inference\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR'] # Comment out if running on GPU\n",
        "!gsutil cp '{results_path}' '{results_file}'\n",
        "if DATASET == 'eval':\n",
        "    beam_size = 4\n",
        "if DATASET == 'test':\n",
        "    beam_size = 8\n",
        "    \n",
        "# Make local backup of original checkpoints file for all models\n",
        "for run in models_to_evaluate:\n",
        "    checkpoint_file = f'{run.model_dir}/checkpoint'\n",
        "    !gsutil cp {checkpoint_file} '{run.get_label()}_checkpoint_backup.tmp'\n",
        "\n",
        "for ckpt in range(0, 10000 + 10000, 10000):\n",
        "    # Create a new dataframe for data if none created\n",
        "    try:\n",
        "        df = pd.read_csv(f'{results_file}', index_col=0)\n",
        "    except FileNotFoundError:\n",
        "        print('creating new dataframe')\n",
        "        df = pd.DataFrame()\n",
        "        \n",
        "    for run in models_to_evaluate:\n",
        "        # Read hparams from model in bucket\n",
        "        checkpoint_file = f'{run.model_dir}/checkpoint'\n",
        "        !gsutil cp $run.model_dir'/hparams.json' .\n",
        "        hparams_json = get_json('hparams.json')\n",
        "\n",
        "        # Check if checkpoint exists in checkpoints file\n",
        "        available_checkpoints = !gsutil cat $checkpoint_file | tr '\\n' ' ' | sed -e 's/[^0-9]/ /g' -e 's/^ *//g' -e 's/ *$//g' | tr -s ' ' | sed 's/ /\\n/g'\n",
        "        available_checkpoints = available_checkpoints[1:]\n",
        "        if not str(ckpt) in available_checkpoints:\n",
        "            print(f'Checkpoint {ckpt} does not exist for {os.path.basename(run.model_dir)}')\n",
        "            continue # Skip iteration if checkpoint does not exists\n",
        "\n",
        "        # Copy checkpoint file locally\n",
        "        !gsutil cat $checkpoint_file > 'checkpoint.tmp'\n",
        "        \n",
        "        # Run sed to replace first line with checkpoint file to run\n",
        "        !sed -i '1s/.*/model_checkpoint_path: \"model.ckpt-'\"{ckpt}\"'\"/' checkpoint.tmp\n",
        "\n",
        "        # Copy altered checkpoint file to bucket\n",
        "        !gsutil cp checkpoint.tmp {checkpoint_file} && echo $(gsutil cat $checkpoint_file) \n",
        "        print(f'{60*\"-\"}\\nRunning inference on {run.model_dir} using checkpoint in {checkpoint_file}\\n')\n",
        "        \n",
        "        # Run decoder for evaluation and restore checkpoint file\n",
        "        decoder(run)\n",
        "        !gsutil cp '{run.get_label()}_checkpoint_backup.tmp' {checkpoint_file}\n",
        "\n",
        "        # Evaluate using ROUGE\n",
        "        !rm -rf {OUTPUT_DIR} && mkdir {OUTPUT_DIR}     \n",
        "        decode_to_file = f'{BUCKET}/generated_summaries/{run.get_label()}_all_outputs.txt'\n",
        "        !gsutil cp '{decode_to_file}' '{run.get_label()}_{ckpt}_all_outputs.txt'    \n",
        "        save_to_separate(f\"{run.get_label()}_{ckpt}_all_outputs.txt\", OUTPUT_DIR, 'file.', output_lim=OUTPUT_LIM)\n",
        "        \n",
        "        # Set up ROUGE155\n",
        "        dict_output = run_rouge_155(run)\n",
        "        print(f'{60*\"_\"}\\n{run.get_label()}\\nCheckpoint: {ckpt}\\n\\tRouge 1 F-score:\\t{dict_output[\"rouge_1_f_score\"]}')\n",
        "        print(f'\\tRouge 2 F-score:\\t{dict_output[\"rouge_2_f_score\"]}')\n",
        "        print(f'\\tRouge L F-score:\\t{dict_output[\"rouge_l_f_score\"]}\\n{60*\"-\"}')\n",
        "        dict_output['max_length'] = TEST_MAX_LENGTH\n",
        "        dict_output['model_max_length'] = hparams_json['max_length']\n",
        "        dict_output['step'] = ckpt\n",
        "        dict_output['num_evaluated'] = MAX_TESTS\n",
        "        dict_output['model'] = run.get_label()\n",
        "        dict_output['output_limit'] = OUTPUT_LIM\n",
        "        df = df.append(pd.Series(dict_output), ignore_index=True)\n",
        "        df = df.drop_duplicates(['model', 'step', 'max_length', 'num_evaluated'], keep='last')\n",
        "        df.to_csv(f'{results_file}')\n",
        "        !gsutil cp '{results_file}' '{results_path}'"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}